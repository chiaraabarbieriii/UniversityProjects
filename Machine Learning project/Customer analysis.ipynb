{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer analysis\n",
    "\n",
    "The intent of this project is to be able to successfully apply classification and clustering on a dataset and be able to analyze and make predictions that are relevant to the goal predefined.\n",
    "\n",
    "For this project I have chosen a dataset which collects the data that is necessary to analyse a customer behavior when making a purchase within a company, so a *Customer Personality Analysis*.\n",
    "\n",
    "The link to find the dataset that was used is:\n",
    "https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis\n",
    "\n",
    "## Goals\n",
    "The goal of this project is to:\n",
    "1. Predict whether offers are an effective method to have a client buy in the store - We will be doing this by using binary classification of the attribute \"Response\", since it tells us whether a customer accepted the offer (1) or refused (0) - classify features values in 1 or 0. It is done in this notebook.\n",
    "2. Segment customers based on their characteristics such as age, income, family situation... -  We will identify distinct customer groups that may have different needs and behaviors by applying clustering techniques. Look at the notebook \"Clustering\" - We will first segment customers based on all the characteristics given by the dataset and afterwards we will only consider their spending habits and Income to segregate them.\n",
    "\n",
    "## Attributes\n",
    "In this dataset the attributes are divided into four different types of categories\n",
    "\n",
    "**People**\n",
    "- ID: Customer's unique identifier\n",
    "- Year_Birth: Customer's birth year\n",
    "- Education: Customer's education level\n",
    "- Marital_Status: Customer's marital status\n",
    "- Income: Customer's yearly household income\n",
    "- Kidhome: Number of children in customer's household\n",
    "- Teenhome: Number of teenagers in customer's household\n",
    "- Dt_Customer: Date of customer's enrollment with the company\n",
    "- Recency: Number of days since customer's last purchase\n",
    "- Complain: 1 if the customer complained in the last 2 years, 0 otherwise\n",
    "\n",
    "**Products**\n",
    "- MntWines: Amount spent on wine in last 2 years\n",
    "- MntFruits: Amount spent on fruits in last 2 years\n",
    "- MntMeatProducts: Amount spent on meat in last 2 years\n",
    "- MntFishProducts: Amount spent on fish in last 2 years\n",
    "- MntSweetProducts: Amount spent on sweets in last 2 years\n",
    "- MntGoldProds: Amount spent on gold in last 2 years\n",
    "\n",
    "**Promotions**\n",
    "- NumDealsPurchases: Number of purchases made with a discount\n",
    "- AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise\n",
    "- AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise\n",
    "- AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise\n",
    "- AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise\n",
    "- AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise\n",
    "- Response: 1 if customer accepted the offer in the last campaign, 0 otherwise\n",
    "\n",
    "**Place of purchase**\n",
    "- NumWebPurchases: Number of purchases made through the company’s website\n",
    "- NumCatalogPurchases: Number of purchases made using a catalogue\n",
    "- NumStorePurchases: Number of purchases made directly in stores\n",
    "- NumWebVisitsMonth: Number of visits to company’s website in the last month\n",
    "\n",
    "The meaning of the attributes was copied from the presentation of the dataset, this presentation can be found in the link given above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries needed for the project\n",
    "In order to develop this project, these libraries are needed (some of the libraries were not used and just added during the development of the project):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the analysis of the dataset\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "\n",
    "# For preprocessing\n",
    "import sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MaxAbsScaler, OrdinalEncoder, StandardScaler, KBinsDiscretizer, add_dummy_feature, LabelEncoder, Binarizer\n",
    "from sklearn.preprocessing import KBinsDiscretizer, add_dummy_feature, LabelEncoder, Binarizer, Normalizer, MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# For splitting in train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# We added all possible methods of libraries to facilitate model selection\n",
    "\n",
    "# Classifiers - Supervised learning\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor # Decision Tree, even if I will probably only use the classification Decision Tree\n",
    "from sklearn.svm import LinearSVC, SVC # Support Vector Machine\n",
    "from sklearn.decomposition import PCA # Dimensionality reduction feature extraction\n",
    "\n",
    "# Classifier - Unsupervised learning\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA # Dimensionality reduction feature extraction\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS # Dimensionality reduction feature selection\n",
    "\n",
    "# To deal with imbalanced classes\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "\n",
    "# Model selection\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_validate, RepeatedStratifiedKFold, HalvingGridSearchCV, HalvingRandomSearchCV\n",
    "from sklearn.model_selection import cross_val_predict, RepeatedKFold, ShuffleSplit, StratifiedShuffleSplit, learning_curve, validation_curve, cross_val_score\n",
    "from random import choice\n",
    "import itertools\n",
    "from imblearn.pipeline import Pipeline as IMBPipeline\n",
    "\n",
    "# Ensemble learning\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier, RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model performance evaluation\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, matthews_corrcoef, roc_curve, get_scorer_names\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score,  precision_recall_curve\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# For the refinement of the model selection\n",
    "from scipy.stats import loguniform, beta, uniform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset analysis\n",
    "\n",
    "To import the dataset we have to use Pandas library, since it allows to visualize the dataset and act upon it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
